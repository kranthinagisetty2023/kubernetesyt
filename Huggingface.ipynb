{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+MeuL9NCAUtmK6E8PdHSI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kranthinagisetty2023/kubernetesyt/blob/main/Huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L5IlNRrA7R2",
        "outputId": "087a136f-d0b7-419e-e96e-8c00cee82f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\"GPU 🔥\" if torch.cuda.is_available() else \"CPU 🥶\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3cCIbfurBKoh",
        "outputId": "7011135d-f3ad-4d2e-99a6-baa705261d35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GPU 🔥'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://huggingface.co/spaces/anzorq/finetuned_diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-cCbdA4BOoP",
        "outputId": "fb7e31a7-6b6a-41ef-8aa0-00ccf46887fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'finetuned_diffusion'...\n",
            "remote: Enumerating objects: 364, done.\u001b[K\n",
            "remote: Counting objects: 100% (364/364), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 364 (delta 227), reused 364 (delta 227), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (364/364), 129.17 KiB | 18.45 MiB/s, done.\n",
            "Resolving deltas: 100% (227/227), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd finetuned_diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYIHBOcQBfsv",
        "outputId": "7da5eaf4-1fc3-4b4b-f47f-936447ff08da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuned_diffusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dEd_10rBjLR",
        "outputId": "b9ccd078-0a37-41cb-c1e8-902f8fef8c12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tnsfw.png  README.md  requirements.txt  style.css  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CFiUXkwBlyX",
        "outputId": "f7c74ce0-797e-4b11-fb14-9006cfabbb58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio -q #gradio isn't required to be mentioned on requirements for apps on HF space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh_7wHpxBo7n",
        "outputId": "1e1c16f3-0da3-451d-8e2c-74c2288a8981"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from email import generator\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "models = [\n",
        "  \"nitrosocke/Arcane-Diffusion\",\n",
        "  \"nitrosocke/archer-diffusion\",\n",
        "  \"nitrosocke/elden-ring-diffusion\",\n",
        "  \"nitrosocke/spider-verse-diffusion\",\n",
        "  \"nitrosocke/modern-disney-diffusion\",\n",
        "  \"hakurei/waifu-diffusion\",\n",
        "  \"lambdalabs/sd-pokemon-diffusers\",\n",
        "  \"yuk/fuyuko-waifu-diffusion\",\n",
        "  \"AstraliteHeart/pony-diffusion\",\n",
        "  \"IfanSnek/JohnDiffusion\",\n",
        "  \"nousr/robo-diffusion\",\n",
        "  \"DGSpitzer/Cyberpunk-Anime-Diffusion\"\n",
        "]\n",
        "\n",
        "prompt_prefixes = {\n",
        "  models[0]: \"arcane style \",\n",
        "  models[1]: \"archer style \",\n",
        "  models[2]: \"elden ring style \",\n",
        "  models[3]: \"spiderverse style \",\n",
        "  models[4]: \"modern disney style \",\n",
        "  models[5]: \"\",\n",
        "  models[6]: \"\",\n",
        "  models[7]: \"\",\n",
        "  models[8]: \"\",\n",
        "  models[9]: \"\",\n",
        "  models[10]: \"\",\n",
        "  models[11]: \"dgs illustration style \",\n",
        "}\n",
        "\n",
        "current_model = models[0]\n",
        "pipe = StableDiffusionPipeline.from_pretrained(current_model, torch_dtype=torch.float16)\n",
        "if torch.cuda.is_available():\n",
        "  pipe = pipe.to(\"cuda\")\n",
        "\n",
        "device = \"GPU 🔥\" if torch.cuda.is_available() else \"CPU 🥶\"\n",
        "\n",
        "def inference(model, img, strength, prompt, guidance, steps, seed):\n",
        "\n",
        "  generator = torch.Generator('cuda').manual_seed(seed) if seed != 0 else None\n",
        "\n",
        "  if img is not None:\n",
        "    return img_inference(model, prompt, img, strength, guidance, steps, generator)\n",
        "  else:\n",
        "    return text_inference(model, prompt, guidance, steps, generator)\n",
        "\n",
        "def text_inference(model, prompt, guidance, steps, generator=None):\n",
        "\n",
        "    global current_model\n",
        "    global pipe\n",
        "    if model != current_model:\n",
        "        current_model = model\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(current_model, torch_dtype=torch.float16)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    prompt = prompt_prefixes[current_model] + prompt\n",
        "    image = pipe(\n",
        "      prompt,\n",
        "      num_inference_steps=int(steps),\n",
        "      guidance_scale=guidance,\n",
        "      width=512,\n",
        "      height=512,\n",
        "      generator=generator).images[0]\n",
        "    return image\n",
        "\n",
        "def img_inference(model, prompt, img, strength, guidance, steps, generator):\n",
        "\n",
        "    global current_model\n",
        "    global pipe\n",
        "    if model != current_model:\n",
        "        current_model = model\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(current_model, torch_dtype=torch.float16)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    prompt = prompt_prefixes[current_model] + prompt\n",
        "    img.resize((512, 512))\n",
        "    image = pipe(\n",
        "        prompt,\n",
        "        init_image=img,\n",
        "        num_inference_steps=int(steps),\n",
        "        strength=strength,\n",
        "        guidance_scale=guidance,\n",
        "        width=512,\n",
        "        height=512,\n",
        "        generator=generator).images[0]\n",
        "    return image\n",
        "\n",
        "\n",
        "css = \"\"\"\n",
        "  <style>\n",
        "  .finetuned-diffusion-div {\n",
        "      text-align: center;\n",
        "      max-width: 700px;\n",
        "      margin: 0 auto;\n",
        "    }\n",
        "    .finetuned-diffusion-div div {\n",
        "      display: inline-flex;\n",
        "      align-items: center;\n",
        "      gap: 0.8rem;\n",
        "      font-size: 1.75rem;\n",
        "    }\n",
        "    .finetuned-diffusion-div div h1 {\n",
        "      font-weight: 900;\n",
        "      margin-bottom: 7px;\n",
        "    }\n",
        "    .finetuned-diffusion-div p {\n",
        "      margin-bottom: 10px;\n",
        "      font-size: 94%;\n",
        "    }\n",
        "    .finetuned-diffusion-div p a {\n",
        "      text-decoration: underline;\n",
        "    }\n",
        "  </style>\n",
        "\"\"\"\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "            <div class=\"finetuned-diffusion-div\">\n",
        "              <div>\n",
        "                <h1>Finetuned Diffusion</h1>\n",
        "              </div>\n",
        "              <p>\n",
        "               Demo for multiple fine-tuned Stable Diffusion models, trained on different styles: <br>\n",
        "               <a href=\"https://huggingface.co/nitrosocke/Arcane-Diffusion\">Arcane</a>, <a href=\"https://huggingface.co/nitrosocke/archer-diffusion\">Archer</a>, <a href=\"https://huggingface.co/nitrosocke/elden-ring-diffusion\">Elden Ring</a>, <a href=\"https://huggingface.co/nitrosocke/spider-verse-diffusion\">Spiderverse</a>, <a href=\"https://huggingface.co/nitrosocke/modern-disney-diffusion\">Modern Disney</a>, <a href=\"https://huggingface.co/hakurei/waifu-diffusion\">Waifu</a>, <a href=\"https://huggingface.co/lambdalabs/sd-pokemon-diffusers\">Pokemon</a>, <a href=\"https://huggingface.co/yuk/fuyuko-waifu-diffusion\">Fuyuko Waifu</a>, <a href=\"https://huggingface.co/AstraliteHeart/pony-diffusion\">Pony</a>, <a href=\"https://huggingface.co/IfanSnek/JohnDiffusion\">John</a>, <a href=\"https://huggingface.co/nousr/robo-diffusion\">Robo</a>, <a href=\"https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion\">Cyberpunk Anime</a>\n",
        "              </p>\n",
        "            </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "\n",
        "        with gr.Column():\n",
        "\n",
        "            model = gr.Dropdown(label=\"Model\", choices=models, value=models[0])\n",
        "            prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Style prefix is applied automatically\")\n",
        "            with gr.Accordion(\"Image to image (optional)\", open=False):\n",
        "              image = gr.Image(label=\"Image\", height=256, tool=\"editor\", type=\"pil\")\n",
        "              strength = gr.Slider(label=\"Strength\", minimum=0, maximum=1, step=0.01, value=0.75)\n",
        "\n",
        "            with gr.Accordion(\"Advanced options\", open=False):\n",
        "              guidance = gr.Slider(label=\"Guidance scale\", value=7.5, maximum=15)\n",
        "              steps = gr.Slider(label=\"Steps\", value=50, maximum=100, minimum=2)\n",
        "              seed = gr.Slider(0, 2147483647, label='Seed (0 = random)', value=0, step=1)\n",
        "\n",
        "            run = gr.Button(value=\"Run\")\n",
        "            gr.Markdown(f\"Running on: {device}\")\n",
        "        with gr.Column():\n",
        "            image_out = gr.Image(height=512)\n",
        "\n",
        "    prompt.submit(inference, inputs=[model, image, strength, prompt, guidance, steps, seed], outputs=image_out)\n",
        "    run.click(inference, inputs=[model, image, strength, prompt, guidance, steps, seed], outputs=image_out)\n",
        "    gr.Examples([\n",
        "        [models[0], \"jason bateman disassembling the demon core\", 7.5, 50],\n",
        "        [models[3], \"portrait of dwayne johnson\", 7.0, 75],\n",
        "        [models[4], \"portrait of a beautiful alyx vance half life\", 10, 50],\n",
        "        [models[5], \"Aloy from Horizon: Zero Dawn, half body portrait, smooth, detailed armor, beautiful face, illustration\", 7, 45],\n",
        "        [models[4], \"fantasy portrait painting, digital art\", 4, 30],\n",
        "    ], [model, prompt, guidance, steps], image_out, text_inference, cache_examples=torch.cuda.is_available())\n",
        "    gr.Markdown('''\n",
        "      Models by [@nitrosocke](https://huggingface.co/nitrosocke), [@Helixngc7293](https://twitter.com/DGSpitzer) and others. ❤️<br>\n",
        "      Space by: [![Twitter Follow](https://img.shields.io/twitter/follow/hahahahohohe?label=%40anzorq&style=social)](https://twitter.com/hahahahohohe)\n",
        "\n",
        "      ![visitors](https://visitor-badge.glitch.me/badge?page_id=anzorq.finetuned_diffusion)\n",
        "    ''')\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(share = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-hxeRvzBslv",
        "outputId": "9c51e59a-093a-4fa2-fe4d-aa634920e117"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q0yHuHSB2aH",
        "outputId": "89bc0a8a-5f1a-41bf-f6e6-7398e0749837"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/finetuned_diffusion/app.py\", line 3, in <module>\n",
            "    from diffusers import StableDiffusionPipeline\n",
            "ModuleNotFoundError: No module named 'diffusers'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kO7KdTIyB_Y3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}